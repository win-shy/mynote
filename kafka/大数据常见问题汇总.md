[TOC]

# HDFS 架构

HDFS 采用 Master / slave 架构，一个 HDFS 集群包括一个单独的 NameNode 和多个 DataNode 节点。

**NameNode** 负责管理整个分布式系统的元数据。主要包括：目录树结构；文件到数据库 Block 的映射关系；DataNode 的状态监控等；这些数据同时保存在 Fsimage和 EditLog。

**Fsimage** ：内存命名空间元数据在外存的镜像文件；

**EditLog**：采用 Write AHead Log 方式，将变化的数据追加到 EditLoa 文件中，防止数据丢失；

**Secondary NameNode**：定期从 NameNode 拉取 FsImage 和 EditLog 进行合并，生成新的文件再回传到 NameNode。减轻 NameNode 的工作压力。

**DataNode**：负责数据块的实际存储和读写工作。当 Client 上传一份大文件，HDFS 会自动将其切割成 128 MB 大小的 Block 文件，同时为了保证数据的可用性，会对每个 Block 文件进行备份。



# Yarn 架构

**ResourceManager**：全局的资源管理器，负责整个应用的资源管理和分配。

**NodeManage**：是每个节点运行资源和任务的管理器，定期向 RM 汇报节点的情况和接收 AM 的Container 的请求。

**ApplicationMaster**：运行在一个 Container 上，与 RM 协商获取资源，并将资源进一步分配给任务；与 NM 通信以启动/停止任务；监控任务的运行状态。

**Container**：资源的抽象，封装了内存，CPU，磁盘，网络等。Yarn 会为每个任务分配 Container 来执行任务。也就是任务的执行具有资源的隔离性。



# MR 过程

MR 包括 Map 阶段 和 Reduce 阶段。

- **Map**

1. input：在进行 Map 计算之前，会对输入的文件进行分片操作，每一个分片 <key,value> 就是一个 Map 任务；
2. Map：根据编写的应用程序来进行相关操作处理；
3. Partition：确定结果发送到哪个 Reduce 端，Partition 的个数等于 Reduce 的个数；
4. Sort：对 Partition 之后的结果进行排序，按照 <key,value,partition> 的格式；数据进入环形缓冲区；
5. Spill：数据进入了环形缓冲区后，在达到阈值时溢出数据，数据将在磁盘上存储；
6. Merge：数据会在本地磁盘合并成一个本地文件，改文件会有一个对应的索引文件；



- Reduce

1. Copy： 通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件 
2. Merge：合并从多个 Map Task 获取的数据；
3. Output：此时将合并的文件转移到内存中，shuffle 阶段正式结束；
4. Reduce：开始执行编写的 Reduce 方式，将结果保留在 HDFS 上。



# YARN 调度

1. MR 程序提交到客户端所在的节点，启动一个 YarnRunner 线程；
2. YarnRunner 向 RM 申请 application；
3. RM 将资源返回给 YarnRunner；
4. MR 程序将需要的资源提交到 HDFS；
5. RM 将用户的请求初始化为 Task；
6. NodeManager 领取 task，创建一个 Container，启动 AM；
7. AM 反向 RM 注册，并获取资源；
8. AM 进行资源的分配，会启动 Container 来运行 task 任务；



# HDFS 写流程

1. Client 调用 DFS 对象的 create 方法，创建一个 FOS 对象；
2. 通过 DFS 对象和集群 NameNode 进行 RPC 远程调用，如果之前没有创建，NameNode 会返回该数据块需要拷贝的 DataNode 的地址信息；
3. 通过 FOS 对象，向 DataNode 进行数据的写入，同时将数据转发到其他 replica 的 DataNode 中。
4. DataNode 确认数据传输完毕，由第一个 DataNode 通知 Client；
5. 完成数据的写入 ，FOS 关闭。DFS 通知 NameNode 文件写入成功，NameNode 将记录写到到 EditLog 中。



# HDFS 读流程

	1. Client 通过DFS 对象和 NameNode 进行 RPC 远程调用，获取 Block 的位置信息；
 	2. NameNode 返回存储的每个块的 DataNode 列表；
 	3. Client 连接 DataNode，并行读取数据；
 	4. 获取了所有 Block，会将 Block 组合形成一个文件。



# HDFS 的文件创建流程

1. Client 通过 ClientProtocol 协议向 RpcServer 发起创建文件的 RPC 请求；
2. FNS 封装了各种 HDFS的操作新街， RpcServer 调用 FNS 创建目录；
3. RpcServer 将 RPC 响应返回给 Client。



# Hadoop 配置文件

- **hadoop-env.sh**: 用于定义hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等；

- **core-site.xml**: 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置；

- **hdfs-site.xml**: HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置；

- **mapred-site.xml**：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置；



# 小文件过多，怎么避免

1. Hadoop 上 HDFS 元数据信息存储在 NameNode 内存中，因此过多的小文件会加重 NameNode 的内存消耗。
2. 同时小文件过多会导致 DataNode 的存储负载压力过重，因为每个小文件都会占用 128 MB的块，这就导致 DataNode 会出现很多的资源浪费。
3. 需要采用合并小文件方式，将上传的文件采取一定的合并策略。



# 启动hadoop集群会分别启动哪些进程,各自的作用

- **NameNode：**
  - 维护文件系统树及整棵树内所有的文件和目录。这些信息永久保存在本地磁盘的两个文件中：命名空间镜像文件、编辑日志文件
  - 记录每个文件中各个块所在的数据节点信息，这些信息在内存中保存，每次启动系统时重建这些信息
  - 负责响应客户端的   数据块位置请求  。也就是客户端想存数据，应该往哪些节点的哪些块存；客户端想取数据，应该到哪些节点取
  - 接受记录在数据存取过程中，datanode节点报告过来的故障、损坏信息

- **SecondaryNameNode(非HA模式)：**
  - 实现namenode容错的一种机制。定期合并编辑日志与命名空间镜像，当namenode挂掉时，可通过一定步骤进行上顶。(**注意 并不是NameNode的备用节点**)
- **DataNode：**
  - 根据需要存取并检索数据块
  - 定期向namenode发送其存储的数据块列表
- **ResourceManager：**
  - 负责Job的调度,将一个任务与一个NodeManager相匹配。也就是将一个MapReduce之类的任务分配给一个从节点的NodeManager来执行。
- **NodeManager：**
  - 运行ResourceManager分配的任务，同时将任务进度向application master报告

- **JournalNode(HA下启用):**
  - 高可用情况下存放namenode的editlog文件



# Hive 的内部表和外部表

内部表：建表时不需要 external 关键字；删除表时，元数据和数据都会被删除；

外部表：建表时需要 external 关键字；删除表时，只会删元数据。



# Hive 四种排序方式

- **order by** 是要对输出的结果进行**全局排序**，这就意味着**只有一个reducer**才能实现（多个reducer无法保证全局有序）但是当数据量过大的时候，效率就很低。如果在严格模式下（hive.mapred.mode=strict）,则必须配合limit使用

- **sort by** 不是全局排序，只是在进入到reducer之前完成排序，只保证了每个reducer中数据按照指定字段的有序性，是**局部排序**。配置mapred.reduce.tasks=[nums]可以对输出的数据执行归并排序。可以配合limit使用，提高性能

- **distribute by** 指的是按照指定的字段划分到不同的**输出 reduce 文件**中，和sort by一起使用时需要注意，
  distribute by必须放在前面

- **cluster by** 可以看做是一个特殊的 distribute by+sort by，它具备二者的功能，但是只能实现倒序排序的方式,不能指定排序规则为asc 或者desc



# Hive 的 metastroe 的三种模式

- **内嵌Derby方式**：这个是Hive默认的启动模式，一般用于单元测试，这种存储方式有一个缺点：在同一时间只能有一个进程连接使用数据库。

- **Local方式**：本地启动 MySQL

- **Remote方式** 远程 MySQL,一般工程常用此种方式



# Hive 的 join

- **内关联（JOIN）**只返回能关联上的结果。

- **左外关联（LEFT [OUTER] JOIN）**以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。

- **右外关联（RIGHT [OUTER] JOIN）**和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。

- **全外关联（FULL [OUTER] JOIN）**以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。

- **（LEFT SEMI JOIN） **以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY也在副表中的记录

- **笛卡尔积关联（CROSS JOIN）**返回两个表的笛卡尔积结果，不需要指定关联键。



# Hive Sql 是怎样解析成MR job的

HQL 生成 **抽象语法树** ，再生成 **Operator Tree**，在形成 **优化的 Operator Tree**，接着生成  MR job，最后通过优化器对 **MR job 进行优化**。



# Hive 的优化操作

- **表拆分**：可以进行大表拆小表、分区表、外部表操作等；
- **MR 优化**：对 Map 和 Reduce 操作进行优化，如运行 map 和 Reduce 的个数
- **并行执行**：通过设置并行度来节约运行时间；
- **JVM 重用**：减少频繁启动和关闭 JVM 的开销，多任务重用一个 JVM；
- **推迟执行**



# 数据倾斜原因

任务进度长时间维持在 99%，查看任务监控页面，发现只有少量 reduce 子任务未完成，因为其处理的数据量和其他 reduce 差异过大。

1. key 分布不均匀；
2. 业务数据本身的特定；
3. 建表的问题；
4. HQL 语句导致



# 避免数据倾斜

- **调教参数**：使 MR job 优化为两个 MR job，第一个做局部聚合，第二个做全局聚合。
- **SQL语句调整**：
  - 当进行 join 操作是，做好相应的裁剪和过滤，保证两表的数据量相对较小
  - 大小表进行 join 时，可以保证小表在前面，先进入内存，在 Map 端完成 Reduce
  - 大表 join 大表，给非空的 key 编程 key_randonNumber，这样可以将倾斜数据分到不同的 reduce 上。
  - count distinct 时，将值为空的情况单独处理。如果是计算 count distinct，可以不用处理，直接过滤，在最后结果中加 1。如果还有其他计算，需要进行 group by，可以先将值为空的记录单独处理，再和其他计算结果进行 union。



# Spark 运行组件

- **Cluster Manager(Master)**：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器

- **Worker节点**：从节点，负责控制计算节点，启动Executor或者Driver。

- **Driver**： 运行Application 的main()函数

- **Executor**：执行器，是为某个Application运行在worker node上的一个进程



# Spark 的运行流程

1. Driver 来开始执行 Application 应用程序，首先会初始化 SparkContext，实例化 SparkContext；
2. SparkContext 实例化后，就会构建 DAGScheduler 和 TaskScheduler；
3. TaskScheduler 会通过对应的后台进程去连接 master，向 Master 注册 Application 应用；
4. Master 收到 Application 注册请求后，会通过资源调度算法，在 worker 节点上为这个 Application 应用启动多个 Executor；
5. Executor 启动之后，会向 TaskScheduler 反向注册上去；
6. 当所有 Executor 都反向注册到 Driver 后，Driver 会结束初始化 SparkContext；

上述流程基本上就完成了资源的分配，接下来就开始实际执行 Application 中的任务了。



7. 当应用程序执行到 action 时，就会创建一个 job，并将 job 提交给 DAGScheduler；
   job 是 Application 应用程序所有任务的集合
8. DAGScheduler 的作用就是：将 job 划分为多个 Stage，并为每个 stage 创建 TaskSet；
9. TaskScheduler 会将 TaskSet 中的 Task 提交到 Executor 上去执行；
10. Executor 接收到 task后，会使用 TaskRunner 来封装 Task，然后从线程池中取出一个线程来执行这个 Task；
11. 每个 Task 针对一个 Partition来执行 Application程序的算子和函数，直到所有操作执行完成。



# Spark 的 join

- **join** 函数会输出两个RDD中key相同的所有项，并将它们的value联结起来，它联结的key要求在两个表中都存在，类似于SQL中的INNER JOIN。但它不满足交换律，a.join(b)与b.join(a)的结果不完全相同，值插入的顺序与调用关系有关。

- **leftOuterJoin** 会保留对象的所有key，而用None填充在参数RDD other中缺失的值，因此调用顺序会使结果完全不同。如下面展示的结果，

- **rightOuterJoin** 与leftOuterJoin基本一致，区别在于它的结果保留的是参数other这个RDD中所有的key。

- **fullOuterJoin** 会保留两个RDD中所有的key，因此所有的值列都有可能出现缺失的情况，所有的值列都会转为Some对象。



# RDD 的特点

- **内存计算**：计算的中间结果存储在内存中而不是磁盘；

- **延迟计算**：所有的 Transformation 都是惰性操作，它们不会立即计算结果，但是它们记住数据集的Transformation 操作，直到 action 的出现，才开始真正的计算；

- **容错性**：在故障时自动重建丢失的数据；

- **不可变性**：数据在计算过程中是不可变的，跨进程共享数据是安全的；

- **分区性**：分区 partition 是 Spark RDD 并行性的基本单元，每个分区都是数据的逻辑分区。在进行 RDD 操作时，实际上是对每个分区的数据进行操作。

- **持久化**：可以指定 RDD 的存储策略，来提高性能；

- **数据本地性**：数据不动代码动。降低数据的流动（磁盘，网络的限制），提高性能。



# 宽依赖与窄依赖

窄依赖：子 RDD 的 partition 与 父 RDD 的 partition 是**一对一**的关系。

宽依赖：子 RDD 的 partition 与 父 RDD 的 partition 是**多对多**的关系。一个子 RDD 的 Partition 可能依赖于多个 父 RDD 的 partition，一个父 RDD 的 Partition 可能被多个子 RDD 的 partition所依赖。在DAGScheduler 中会产生 stage 的切分。



# Map 与 FlatMap 的区别

Map 与 Flatmap 都是窄依赖。

Map ：输入一个元素，就会返回一个元素。

FlatMap：输入一个元素，返回一个集合。



# Spark 缓存级别

- NONE :什么类型都不是
- DISK_ONLY：磁盘
- DISK_ONLY_2：磁盘；双副本
- MEMORY_ONLY： 内存；反序列化；把RDD作为反序列化的方式存储，假如RDD的内容存不下，剩余的分区在以后需要时会重新计算，不会刷到磁盘上。
- MEMORY_ONLY_2：内存；反序列化；双副本
- MEMORY_ONLY_SER：内存；序列化；这种序列化方式，每一个partition以字节数据存储，好处是能带来更好的空间存储，但CPU耗费高
- MEMORY_ONLY_SER_2 : 内存；序列化；双副本
- MEMORY_AND_DISK：内存 + 磁盘；反序列化；双副本；RDD以反序列化的方式存内存，假如RDD的内容存不下，剩余的会存到磁盘
- MEMORY_AND_DISK_2 : 内存 + 磁盘；反序列化；双副本
- MEMORY_AND_DISK_SER：内存 + 磁盘；序列化 
- MEMORY_AND_DISK_SER_2：内存 + 磁盘；序列化；双副本jh



# RDD 的 懒加载

Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Acion 操作的时候才会真正触发运算,这也就是懒加载.



# Spark 的部署方式

- **Standalone** 即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统
- **Spark on Yarn** 资源调度使用 yarn 来取代
- **Spark on Mesos**： 资源调度使用 Mesos 来取代



# Cluster & Client 模式

1. Yarn-cluster 适用于生产环境。而 yarn-client 适用于交互和调试，也就是希望快速地看到 application 的输出.
2. yarn-cluster 和 yarn-client 模式的区别其实就是 **Application Master 进程** 的区别，yarn-cluster 模式下，driver 运行在 AM(Application Master)中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。然而 yarn-cluster 模式不适合运行交互类型的作业。而 yarn-client 模式下，Application Master 仅仅向 YARN 请求 executor，Client 会和请求的container 通信来调度他们工作，也就是说 Client 不能离开。

- Client 关闭命令行窗口，相当于取消程序运行。同时可以在控制台上看到运行输出内容；

- Cluster 关闭命令行窗口，不影响程序运行。控制台上看不到输出内容。



# stage 的划分依据

**stage的划分依据就是看是否产生了shuflle(即宽依赖),遇到一个shuffle操作就划分为前后两个stage.**



# Spark 为什么放弃 akka 用netty

1. 不同 Akka 版本之间无法相互通信；
2. 可能会导致与编写代码的 Akka 配置冲突；
3. 需要等待 Akka 上游更新；



# Spark 内存管理

spark的内存结构分为3大块: **storage/execution/系统自留**

- **storage 内存**：用于缓存 RDD、展开 partition、存放 Direct Task Result、存放广播变量。在 Spark Streaming receiver 模式中，也用来存放每个 batch 的 blocks

- **execution 内存**：用于 shuffle、join、sort、aggregation 中的缓存、buffer

- **系统自留**: 在 spark 运行过程中使用：比如序列化及反序列化使用的内存，各个对象、元数据、临时变量使用的内存，函数调用使用的堆栈等；作为误差缓冲：由于 storage 和 execution 中有很多内存的使用是估算的，存在误差。当 storage 或 execution 内存使用超出其最大限制时，有这样一个安全的误差缓冲在可以大大减小 OOM 的概率



# Spark shuffle 数据倾斜

 数据倾斜的定位：通过 WEBUI 来进行查看运行情况而判断。

**过滤引起数据倾斜的 key**

**提高 shuffle 操作的并行度**

**对数据倾斜 key使用随机数，实现两阶段聚合**

**将 hash shuffle join 转换成 map join（广播）**

**使用 Partitioner 优化 hash shuffle join**



# Spark 优化

**避免重复创建 RDD，尽可能复用 RDD**

**对重复使用的 RDD 进行持久化**

**尽量避免使用会触发 shuffle 的算子**

**使用高性能算子**

**将变量广播出去**

**使用 kryo 序列化方式来优化序列化性能**

**使用优化的数据结构**



# spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行

Driver Program是用户编写的提交给Spark集群执行的application，它包含两部分

- **作为驱动**： Driver与Master、Worker协作完成application进程的启动、DAG划分、计算任务封装、计算任务分发到各个计算节点(Worker)、计算资源的分配等。
- **计算逻辑本身**，当计算任务在Worker执行时，执行计算逻辑完成application的计算任务



一般来说transformation算子均是在worker上执行的,其他类型的代码在driver端执行



# HBase 架构

**Hbase主要包含HMaster/HRegionServer/Zookeeper**

- **HMaster 负责管理Region的位置, DDL(新增和删除表结构)**
  - 协调RegionServer
  - 在集群处于数据恢复或者动态调整负载时,分配Region到某一个RegionServer中
  - 管控集群,监控所有Region Server的状态
  - 提供DDL相关的API, 新建(create),删除(delete)和更新(update)表结构.

- **Zookeeper 负责维护和记录整个Hbase集群的状态**
  - zookeeper探测和记录Hbase集群中服务器的状态信息.如果zookeeper发现服务器宕机,它会通知Hbase的master节点.
- **HRegionServer 负责实际数据的读写. 当访问数据时, 客户端直接与RegionServer通信.**
  -   HBase的表根据Row Key的区域分成多个Region, 一个Region包含这这个区域内所有数据. 而Region server负责管理多个Region, 负责在这个Region server上的所有region的读写操作. 



# Rowkey 的设计

- 长度原则，不超过 16 KB
- 散列原则，防止热点现象；
- 唯一性原则。



# HBase 的特点

半结构化或非结构化数据；

记录很稀疏：；

多版本号数据；

仅要求最终一致性；

高可用和海量数据以及很大的瞬间写入量；



# HBase 中 zookeeper 的作用

1. HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。
   配置HBase高可用，只需要启动两个HMaster，让Zookeeper自己去选择一个Master Acitve即可

2. zk的在这里起到的作用就是用来管理master节点,以及帮助hbase做master选举
3. 保证 HBase 的高可用性



# HMaster宕机的时候,哪些操作还能正常工作

对表内数据的增删查改是可以正常进行的,因为hbase client 访问数据只需要通过 zookeeper 来找到 rowkey 的具体 region 位置即可. 但是对于**创建表/删除表**等的操作就无法进行了,因为这时候是需要HMaster介入, 并且region的拆分,合并,迁移等操作也都无法进行了



# HBase 的写流程

1. Client 首先访问 zookeeper，通过相关信息找到 RegionServer；
2. 连接 RegionServer，将数据写到 WAL（HLog）中，WAL 主要用于数据的恢复；
3. 一旦数据被写入到 WAL 之后，接着将数据更新到 MemStore，这时会向 Client 发送 ACK 确认；
4. MemStore 达到一定的阈值之后，将数据刷盘到磁盘的 StoreFile。



# HBase 的读流程

1. Client 首先从 zookeeper 读取 HBase 的元数据表所在的 RegionServer；
2. 本地缓存元数据表，确定待检索的 rowkey 所在的 RegionServer；
3. 根据所在的 RegionServer，Client 向 RegionServer 发送读请求；
4. RegionServer 收到请求，将查询结果返回给客户端。



# Kafka 架构

- **Producer**：消息生产者

- **Broker**：每个Broker里包含了不同Topic的不同Partition，Partition中包含了有序的消息

- **Consumer**：消息消费者



# kafka 实现高吞吐的原理

- 读写文件依赖OS文件系统的页缓存，而不是在JVM内部缓存数据，利用OS来缓存，内存利用率高
- sendfile技术（零拷贝），避免了传统网络IO四步流程
- 支持End-to-End的压缩
- 顺序IO以及常量时间get、put消息
- Partition 可以很好的横向扩展和提供高并发处理



# Kafka保证不重复消费

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。
- 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

保证幂等性。



# kafka怎样保证不丢失消息

**消费端弄丢了数据**：处理完数据，手动提交offset。

**Broker丢失数据**：无限重试；保证副本同步；副本大小；

**生产端弄丢了数据**：acks = all



# kafka 与 spark streaming 集成,如何保证 exactly once 语义

1. Spark Streaming使用Direct模式对接上游kafka

2. 多次尝试总是写入相同的数据
3. 使用事务更新



# Ack 有哪几种, 生产中怎样选择?

ack=0/1/-1的不同情况：

- Ack = 0：producer不等待broker的ack，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；

- Ack = 1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；

- Ack = -1：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack，数据一般不会丢失，延迟时间长但是可靠性高。

**生产中主要以 Ack=-1为主,如果压力过大,可切换为Ack=1. Ack=0的情况只能在测试中使用.**



# Kafka 如何清理过期数据

- 清理超过指定时间清理

- 超过指定大小后，删除旧的消息



# zookeeper 在 kafka 的作用

1. kafka的元数据都存放在zk上面,由zk来管理
2. 0.8之前版本的kafka, consumer的消费状态，group的管理以及 offset的值都是由zk管理的,现在offset会保存在本地topic文件里
3. 负责borker的lead选举和管理



# kafka 可以脱离 zookeeper 单独使用吗

kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。



# zookeeper 有什么功能

数据发布/订阅；负载均衡；命名服务；分布式协调/通知；集群管理；Master；分布式锁；分布式队列

- 保证数据的一致性



# zk 有几种部署模式

 集群模式，单机模式， 伪集群模式



# zk是怎样保证主从节点的状态同步

zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 zab 协议。 zab 协议有两种模式，分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。



# zk的通知机制

客户端端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些客户端会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变



#  **spark 中的partition，task，stage（Taskset）等关系**

1. 在HDFS上存储的文件File一般有多个块，成为block。大小一般128M

2. Task 与 partition是对等的。 一个partition 应对 一个task作业。输入的文件被划分成多个partition进行计算。

3. 一个executor有几个core，则有几个task被并行执行（Taskset）。一个core在一个时间内只能执行一个task任务。

4. 一个物理节点可以有多个worker；一个worker可以开启多个 executor。

5. Task被执行的并发度 = Executor数目 * 每个Executor核数。

6. stage 是以 shuffle为界限划分。一次shuffle是一次stage，所以stage次数 = shuffle次数 + 1。

7. 一个job 由多组task组成，每组任务被称为一个stage



# Spark stream 的batch duration，window duration， slide duration  

1. Batch Duration： 批处理间隔， 是指Spark streaming以多少时间间隔为单位提交任务逻辑

2. Window Duration：当前一个窗口处理数据的时间跨度。控制每次计算最近的多少个批次的数据。（每次计算的所占窗口的数据量）

3. Slide window：控制着计算的频率。用来控制对新的 DStream 进行计算的间隔。（多少窗口时间开始计算）



# Kafka Producer 三种发送消息的方式  

- Fire-and-forget --- 此方法用来发送消息到broker，不关注消息是否成功到达。大部分情况下，消息会成功到达broker，因为kafka是高可用的，producer会自动重试发送。但是，还是会有消息丢失的情况；

- Synchronous Send(同步发送) --- 发送一个消息，send()方法返回一个Future对象，使用此对象的get()阻塞方法可以查看send()方法是否执行成功。

- Asynchronous Send(异步发送) --- 以回调函数的形式调用send()方法，当收到broker的响应，会触发回调函数执行(Callback)。



# Kafka的负载均衡是如何实现的  

由于Leader的主要角色是执行分区的所有读和写请求的任务，而Follower则是被动地复制Leader。因此，当Leader失败的时候，一个Follower接管了Leader的角色。整个过程确保了服务器的负载平衡。



# Kafka和Flume的区别是什么  

- kafka和Flume都是数据采集系统，kafka是分布式消息中间件，自带存储，提供push和pull存取数据功能。Flume分为agent（数据采集器）,collector（数据简单处理和写入）,storage（存储器）三部分，每一部分都是可以定制的。

- kafka做数据缓存更为合适，但是 Flume数据采集部分做的很好，可以定制很多数据源，减少开发量。所以比较流行Flume+kafka模式。
- push（flume） pull（kafka）



# 如何进行Kafka调优  

Kafka调优可以从三方面考虑：

- Kafka Producer调优(消息压缩、批次大小、同步或异步发送)，结合Producer配置项一起学习。

- Kafka Brokers 调优（设置合适的topic partition个数和副本个数，一般情况下，分区个数和副本个数与kafka集群节点个数相同。同时要考虑消费者的个数以及物理磁盘个数。）

-  Kafka Consumers调优(为消费组中增加消费者，但是不能大于分区数。选择合适的消费语义。)



# HBase 避免热点问题

某一段时间内，Hbase读写集中在少部分Region上，负载明显过大，其他RegionServer闲置，叫做热点现象．

方案：RowKey设计，预分区，列簇设计，索引表



# HBase 性能调优

- 预分区：对应数据需要写到对应的region中，这个也解决了数据倾斜

- Rowkey优化：（QA：Hbase表设计关键点）

- Column优化：列族的名称和列的描述名字要短，同一张的ColumnFamilly不要超过3个

- Schema优化：宽表和高表（业务场景平衡）事务性、查询性能 、损耗资源等等



# SparkSQL程序执行过程

1. 先写Dataset API SQL代码
2. 如果代码没有编译错误，Spark会将代码转换为逻辑计划
3. Spark会将逻辑计划转换为物理计划，会对代码进行优化(catalyst优化器)
4. Spark会执行物理计划( RDD )



# HBase，Redis, MongDB 的应用场景

- MongoDB是高性能、无模式的文档型数据库，支持二级索引，非常适合文档化格式的存储及查询。MongoDB的官方定位是通用数据库，确实和MySQL有些像，现在也很流行，但它还是有事务、join等短板，在事务、复杂查询应用下无法取代关系型数据库。

- Redis是内存型Key/Value系统，读写性能非常好，支持操作原子性，很适合用来做高速缓存。

- HBase存储容量大，一个表可以容纳上亿行、上百万列，可应对超大数据量要求扩展简单的需求。Hadoop的无缝集成，让HBase的数据可靠性和海量数据分析性能（MapReduce）值得期待。



# Redis 主从同步步骤

1. slave 服务器连接 master 服务器, 并发送 psync 同步码.

2. master 服务器收到 psync 同步码, 执行 bgsave 生成snapshot(内存中,rdb), 并在缓存区记录从当前开始执行的所有写命令. 

3. master 服务器在 bgsave 命令执行完成后, 将 snapshot 发送到 slave 服务器, slave 载入快照.

4. slave 服务器收到快照, 载入到内存中. 将自己的数据库状态更新至主服务器执行 BGSAVE 命令时的数据库状态. (rdb属于内存性的文件, 所以每次都会丢弃旧 rdb)

5. master 服务器发送完成 snapshot 后, 向 slave 服务器发送缓存区的写命令.(注意时间点)

6. salve 服务器执行这些写命令，将自己的数据库状态更新至主服务器数据库当前所处的状态。



# Redis 单线程模型每秒万级别处理能力的原因

1. 纯内存访问；
2. IO 多路复用；
3. 单线程避免线程切换和竞态产生；